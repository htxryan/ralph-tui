# Implementation Plan

You are tasked with creating detailed implementation plans autonomously. You should be skeptical, thorough, and use your best judgment to produce high-quality technical specifications.

## CRITICAL: AUTONOMOUS EXECUTION - NEVER ASK QUESTIONS

**You MUST complete this command autonomously without asking clarifying questions.**

- **NEVER** ask the user questions during execution
- **NEVER** pause and wait for user input, confirmation, or feedback
- **NEVER** present options and ask "which approach do you prefer?"
- **ALWAYS** make your best judgment when decisions or ambiguities arise
- **ALWAYS** produce a complete, valid implementation plan as output
- If requirements are ambiguous, make a reasonable assumption and document it in the plan
- If multiple approaches exist, choose the best one and explain your reasoning
- If you're unsure about scope, make a clear decision and note it in "What We're NOT Doing"
- The user will critique your output AFTER you produce it - do not pre-emptively ask for guidance
- Getting stuck waiting for user input is UNACCEPTABLE - always move forward to produce the plan

## Arguments

This command accepts a required argument:

- **$ARGUMENTS**: The task ID (e.g., "my-task") corresponding to a task directory

Parse the arguments to extract:
- **task_id**: The task identifier (required) - corresponds to `.ai-docs/thoughts/plans/<task_id>/`
- **additional_context**: Any additional instructions or file paths after the task_id

If no task_id is provided, respond:
```
Error: Task ID required.

Usage: /refine/create_plan <task-id> [context or file path]

Example: /refine/create_plan my-feature
Example: /refine/create_plan my-feature .ai-docs/design/product-brief.md

The task ID corresponds to a task directory containing:
- Input:  .ai-docs/thoughts/plans/<task-id>/task.md (required - defines the task)
- Input:  .ai-docs/thoughts/plans/<task-id>/research.md (optional - from research_codebase)
- Output: .ai-docs/thoughts/plans/<task-id>/plan.md (generated by this command)
```
Then stop and wait for correct input.

## Initial Response

When this command is invoked with a valid task_id:

1. **Validate the task file exists**:
   - Check for `.ai-docs/thoughts/plans/<task_id>/task.md`
   - If it does NOT exist, respond:
     ```
     Error: Task file not found.

     Expected: .ai-docs/thoughts/plans/<task_id>/task.md

     Please create the task file first with your task definition, then run this command again.
     ```
     Then stop and wait.

2. **Read the task file FULLY**:
   - Read `.ai-docs/thoughts/plans/<task_id>/task.md` WITHOUT limit/offset
   - This file defines what needs to be implemented - understand it completely

3. **Check for existing research**:
   - Look for `.ai-docs/thoughts/plans/<task_id>/research.md`
   - If it exists, read it FULLY - this provides the foundation for planning
   - If it doesn't exist, note this and offer to proceed without it

4. **Check if additional parameters were provided**:
   - If a file path or reference was provided as additional context, read those files FULLY
   - Begin the planning process

5. **If no research exists**, note it and proceed anyway:
   ```
   I'll create an implementation plan for task: <task_id>

   Task file: .ai-docs/thoughts/plans/<task_id>/task.md
   Output: .ai-docs/thoughts/plans/<task_id>/plan.md

   Based on the task file, I understand we need to:
   [Summary of what task.md describes]

   Note: No existing research found at .ai-docs/thoughts/plans/<task_id>/research.md
   Proceeding with planning using task.md and codebase analysis...
   ```

   Then proceed with the planning process, gathering context as needed.

6. **If research exists**, proceed directly:
   ```
   I'll create an implementation plan for task: <task_id>

   Task file: .ai-docs/thoughts/plans/<task_id>/task.md
   Research: .ai-docs/thoughts/plans/<task_id>/research.md
   Output: .ai-docs/thoughts/plans/<task_id>/plan.md

   Based on the task file and research, I understand we need to:
   [Summary combining task.md and research.md insights]

   Beginning planning process...
   ```

   Then proceed with the planning process using the task and research as foundation.

## Process Steps

### Step 1: Context Gathering & Initial Analysis

1. **Read all mentioned files immediately and FULLY**:
   - Task document: `.ai-docs/thoughts/plans/<task_id>/task.md` (REQUIRED - already read)
   - Research document: `.ai-docs/thoughts/plans/<task_id>/research.md` (if exists)
   - Design docs (e.g., `.ai-docs/design/product-brief.md`)
   - Related implementation plans
   - Any JSON/data files mentioned
   - **IMPORTANT**: Use the Read tool WITHOUT limit/offset parameters to read entire files
   - **CRITICAL**: DO NOT spawn sub-tasks before reading these files yourself in the main context
   - **NEVER** read files partially - if a file is mentioned, read it completely

2. **Spawn initial research tasks to gather context** (if more context needed):
   Before asking the user any questions, use specialized agents to research in parallel:

   - Use the **codebase-locator** agent to find all files related to the task
   - Use the **codebase-analyzer** agent to understand how the current implementation works
   - If relevant, use the **thoughts-locator** agent to find any existing thoughts documents about this feature

   These agents will:
   - Find relevant source files, configs, and tests
   - Identify the specific directories to focus on
   - Trace data flow and key functions
   - Return detailed explanations with file:line references

3. **Read all files identified by research tasks**:
   - After research tasks complete, read ALL files they identified as relevant
   - Read them FULLY into the main context
   - This ensures you have complete understanding before proceeding

4. **Analyze and verify understanding**:
   - Cross-reference the task.md requirements with actual code
   - Identify any discrepancies or misunderstandings
   - Note assumptions that need verification
   - Determine true scope based on codebase reality

5. **Document understanding and make decisions**:
   Synthesize your research findings and make decisions autonomously:
   - Summarize what you understand needs to be done
   - Document key findings from the codebase
   - When facing ambiguity, make your best judgment and document it
   - When multiple approaches exist, choose the best one and explain your reasoning in the plan

   Do NOT ask questions. If something is genuinely unclear, make a reasonable decision and document your assumption in the plan's "Assumptions Made" section so the user can adjust if needed.

### Step 2: Research & Discovery

Continue researching to fill gaps in your understanding:

1. **Create a research todo list** using TodoWrite to track exploration tasks

3. **Spawn parallel sub-tasks for comprehensive research**:
   - Create multiple Task agents to research different aspects concurrently
   - Use the right agent for each type of research:

   **For deeper investigation:**
   - **codebase-locator** - To find more specific files (e.g., "find all files that handle [specific component]")
   - **codebase-analyzer** - To understand implementation details (e.g., "analyze how [system] works")
   - **codebase-pattern-finder** - To find similar features we can model after

   **For historical context:**
   - **thoughts-locator** - To find any research, plans, or decisions about this area
   - **thoughts-analyzer** - To extract key insights from the most relevant documents

   Each agent knows how to:
   - Find the right files and code patterns
   - Identify conventions and patterns to follow
   - Look for integration points and dependencies
   - Return specific file:line references
   - Find tests and examples

3. **Wait for ALL sub-tasks to complete** before proceeding

4. **Synthesize findings and choose approach**:
   Based on your research, determine the best implementation approach:
   - Evaluate any design options based on what you learned
   - Choose the approach that best fits the codebase patterns and constraints
   - Document your reasoning in the plan
   - If there were multiple viable approaches, explain why you chose one over the others

### Step 3: Plan Structure Development

Proceed directly to structuring the plan:

1. **Determine plan structure**:
   Based on your research and chosen approach, structure the implementation into logical phases:
   - Each phase should accomplish a specific, testable goal
   - Order phases by dependencies and logical progression
   - Include appropriate granularity for the scope of work

### Step 4: Detailed Plan Writing

Proceed directly to writing the plan:

1. **Write the plan** to `.ai-docs/thoughts/plans/<task_id>/plan.md`

2. **Use this template structure**:

````markdown
---
date: [Current date in YYYY-MM-DD format]
task_id: "<task_id>"
topic: "[Feature/Task Name]"
tags: [plan, implementation, relevant-components]
status: draft
---

# [Feature/Task Name] Implementation Plan

**Task ID**: <task_id>

## Overview

[Brief description of what we're implementing and why]

## Task Definition

[Summary of what task.md specifies - the original requirements]

## Current State Analysis

[What exists now, what's missing, key constraints discovered]

## Desired End State

[A Specification of the desired end state after this plan is complete, and how to verify it]

### Key Discoveries:
- [Important finding with file:line reference]
- [Pattern to follow]
- [Constraint to work within]

## What We're NOT Doing

[Explicitly list out-of-scope items to prevent scope creep]

## Implementation Approach

[High-level strategy and reasoning]

## Phase 1: [Descriptive Name]

### Overview
[What this phase accomplishes]

### Changes Required:

#### 1. [Component/File Group]
**File**: `path/to/file.ext`
**Changes**: [Summary of changes]

```[language]
// Specific code to add/modify
```

### Success Criteria:

#### Automated Verification:
- [ ] Build passes: `pnpm build`
- [ ] Unit tests pass: `pnpm test:run`
- [ ] Type checking passes: `pnpm typecheck`
- [ ] Linting passes (if configured)

#### Manual Verification:
- [ ] Feature works as expected when tested in terminal
- [ ] No regressions in related features
- [ ] Edge case handling verified manually

**Implementation Note**: After completing this phase and all automated verification passes, pause here for manual confirmation from the human that the manual testing was successful before proceeding to the next phase.

---

## Phase 2: [Descriptive Name]

[Similar structure with both automated and manual success criteria...]

---

## Testing Strategy

### Unit Tests:
- [What to test]
- [Key edge cases]

### Integration Tests:
- [End-to-end scenarios]

### Manual Testing Steps:
1. [Specific step to verify feature]
2. [Another verification step]
3. [Edge case to test manually]

## Performance Considerations

[Any performance implications or optimizations needed]

## References

- Task definition: `.ai-docs/thoughts/plans/<task_id>/task.md`
- Research doc: `.ai-docs/thoughts/plans/<task_id>/research.md`
- Design doc: `.ai-docs/design/[relevant].md`
- Similar implementation: `[file:line]`
````

### Step 5: Finalize and Present

1. **Confirm plan completion**:
   ```
   I've created the implementation plan at:
   .ai-docs/thoughts/plans/<task_id>/plan.md

   The plan includes:
   - [X phases covering the full implementation]
   - [Key assumptions documented]
   - [Automated and manual verification criteria]

   Next step: Run /refine/capture <task_id> to create a Vibe Kanban issue.
   ```

   Do NOT ask for feedback. The user will review and provide feedback if needed.

2. **If user later provides feedback**, iterate by:
   - Adding missing phases
   - Adjusting technical approach
   - Clarifying success criteria (both automated and manual)
   - Adding/removing scope items

## Important Guidelines

1. **Be Skeptical (Internally)**:
   - Internally question vague requirements before proceeding
   - Identify potential issues early through research
   - Think through "why" and "what about" - then make decisions
   - Don't assume - verify with code, then document your findings

2. **Be Autonomous**:
   - Produce the complete plan in one execution
   - Make decisions confidently based on your research
   - Document assumptions clearly so user can adjust if needed
   - The user will provide feedback AFTER seeing the output

3. **Be Thorough**:
   - Read task.md and all context files COMPLETELY before planning
   - Research actual code patterns using parallel sub-tasks
   - Include specific file paths and line numbers
   - Write measurable success criteria with clear automated vs manual distinction

4. **Be Practical**:
   - Focus on incremental, testable changes
   - Consider migration and rollback
   - Think about edge cases
   - Include "what we're NOT doing"

5. **Track Progress**:
   - Use TodoWrite to track planning tasks
   - Update todos as you complete research
   - Mark planning tasks complete when done

6. **No Open Questions in Final Plan**:
   - If you encounter open questions during planning, research further to resolve them
   - Make your best judgment on any remaining ambiguities - do NOT ask for clarification
   - Document any significant assumptions in an "Assumptions Made" section
   - The implementation plan must be complete and actionable
   - Every decision must be made before finalizing the plan - you make them, don't defer to the user

## Success Criteria Guidelines

**Always separate success criteria into two categories:**

1. **Automated Verification** (can be run by execution agents):
   - Commands that can be run: `pnpm test`, `pnpm build`, etc.
   - Specific files that should exist
   - Code compilation/type checking
   - Automated test suites

2. **Manual Verification** (requires human testing):
   - UI/UX functionality in the terminal
   - Performance under real conditions
   - Edge cases that are hard to automate
   - User acceptance criteria

**Format example:**
```markdown
### Success Criteria:

#### Automated Verification:
- [ ] TypeScript compiles: `pnpm build`
- [ ] All unit tests pass: `pnpm test:run`
- [ ] Type checking passes: `pnpm typecheck`
- [ ] No linting errors (if configured)

#### Manual Verification:
- [ ] New feature appears correctly in the TUI
- [ ] Keyboard shortcuts work as expected
- [ ] Performance is acceptable with large JSONL files
- [ ] Error messages are user-friendly
```

## Common Patterns

### For New Components:
- Start with types in `lib/types.ts`
- Create component file in appropriate `components/` subdirectory
- Add tests alongside component
- Export from index if needed

### For New Hooks:
- Create hook in `hooks/` directory
- Follow existing naming conventions (`use-*.ts`)
- Add to `hooks/index.ts` export
- Include comprehensive tests

### For New Features:
- Research existing patterns first
- Start with data model/types
- Build React components
- Connect with hooks
- Test thoroughly

### For Refactoring:
- Document current behavior
- Plan incremental changes
- Maintain backwards compatibility
- Include migration strategy

## Sub-task Spawning Best Practices

When spawning research sub-tasks:

1. **Spawn multiple tasks in parallel** for efficiency
2. **Each task should be focused** on a specific area
3. **Provide detailed instructions** including:
   - Exactly what to search for
   - Which directories to focus on
   - What information to extract
   - Expected output format
4. **Be specific about directories**:
   - For TUI code, specify `.ralph/tui/src/`
   - For components, specify `.ralph/tui/src/components/`
   - For hooks, specify `.ralph/tui/src/hooks/`
5. **Specify read-only tools** to use
6. **Request specific file:line references** in responses
7. **Wait for all tasks to complete** before synthesizing
8. **Verify sub-task results**:
   - If a sub-task returns unexpected results, spawn follow-up tasks
   - Cross-check findings against the actual codebase
   - Don't accept results that seem incorrect

Example of spawning multiple tasks:
```python
# Spawn these tasks concurrently:
tasks = [
    Task("Research component patterns", component_research_prompt),
    Task("Find hook patterns", hook_research_prompt),
    Task("Investigate type definitions", types_research_prompt),
    Task("Check test patterns", test_research_prompt)
]
```

## Example Execution Flow

```
User: /refine/create_plan my-feature
Assistant: I'll create an implementation plan for task: my-feature

Task file: .ai-docs/thoughts/plans/my-feature/task.md
Research: .ai-docs/thoughts/plans/my-feature/research.md
Output: .ai-docs/thoughts/plans/my-feature/plan.md

Based on the task file and research, I understand we need to:
[Summary of task.md requirements]

Beginning planning process...

[Spawns research agents, gathers context, makes decisions]

I've created the implementation plan at:
.ai-docs/thoughts/plans/my-feature/plan.md

The plan includes:
- 3 phases covering the full implementation
- Key assumptions documented
- Automated and manual verification criteria

Next step: Run /refine/capture my-feature to create a Vibe Kanban issue.
```

## Codebase Context

This is the Ralph TUI codebase - a terminal user interface for monitoring autonomous AI coding agents. Key areas:

- **Main TUI application**: `.ralph/tui/src/` (Ink + React + TypeScript)
- **Components**: `.ralph/tui/src/components/`
- **Hooks**: `.ralph/tui/src/hooks/`
- **Types**: `.ralph/tui/src/lib/types.ts`
- **AI documentation**: `.ai-docs/` (design docs, ADRs, prompts, thoughts)

Development commands (run from `.ralph/tui/`):
- `pnpm build` - Compile TypeScript
- `pnpm test:run` - Run tests once
- `pnpm typecheck` - Type checking only
- `pnpm dev:tsx` - Run with hot reload

## Workflow

This command is part of the refinement workflow:
1. `/refine/research_codebase <task-id>` - Research and document (reads task.md, creates research.md)
2. **`/refine/create_plan <task-id>`** - Create implementation plan (reads task.md + research.md, creates plan.md)
3. `/refine/capture <task-id>` - Capture to Vibe Kanban issue
